"""
ë©´ì ‘ ìŠ¤ì¼€ì¤„ë§ ì‹œìŠ¤í…œ API
ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•  ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
"""
from typing import Dict, List, Optional, Union, Tuple, Any
from datetime import datetime, time, timedelta
import logging
import pandas as pd

from .types import (
    DatePlan, GlobalConfig, MultiDateResult, PrecedenceRule,
    ActivityMode, ActivityType, Activity, Room, DateConfig,
    ProgressCallback, SchedulingContext
)
from .multi_date_scheduler import MultiDateScheduler
from .single_date_scheduler import SingleDateScheduler


def schedule_interviews(
    date_plans: Dict[str, Dict],
    global_config: Dict,
    rooms: Dict,
    activities: Dict,
    logger: Optional[logging.Logger] = None
) -> Union[MultiDateResult, Dict]:
    """
    ë©´ì ‘ ìŠ¤ì¼€ì¤„ë§ ë©”ì¸ API
    
    Args:
        date_plans: ë‚ ì§œë³„ ì„¤ì •
            {
                "2025-07-01": {
                    "jobs": {"JOB01": 23, "JOB02": 20},
                    "selected_activities": ["í† ë¡ ë©´ì ‘", "ë°œí‘œë©´ì ‘"],
                    "overrides": {...}  # Optional
                }
            }
            
        global_config: ì „ì—­ ì„¤ì •
            {
                "precedence": [("ë°œí‘œì¤€ë¹„", "ë°œí‘œë©´ì ‘", 0, True)],
                "operating_hours": {"start": "09:00", "end": "17:30"},
                "batched_group_sizes": {"í† ë¡ ë©´ì ‘": [4, 6]},
                "global_gap_min": 5,
                "max_stay_hours": 8
            }
            
        rooms: ë°© ì„¤ì •
            {
                "í† ë¡ ë©´ì ‘ì‹¤": {"count": 2, "capacity": 6},
                "ë°œí‘œë©´ì ‘ì‹¤": {"count": 2, "capacity": 1}
            }
            
        activities: í™œë™ ì„¤ì •
            {
                "í† ë¡ ë©´ì ‘": {
                    "mode": "batched",
                    "duration_min": 30,
                    "room_type": "í† ë¡ ë©´ì ‘ì‹¤",
                    "min_capacity": 4,
                    "max_capacity": 6
                }
            }
            
    Returns:
        ìŠ¤ì¼€ì¤„ë§ ê²°ê³¼ (ì„±ê³µì‹œ DataFrame í¬í•¨)
    """
    
    # 1. ì…ë ¥ ë°ì´í„° ë³€í™˜
    try:
        # DatePlan ê°ì²´ë“¤ë¡œ ë³€í™˜
        date_plan_objects = {}
        for date_str, plan_data in date_plans.items():
            date = datetime.strptime(date_str, "%Y-%m-%d")
            date_plan_objects[date] = DatePlan(
                date=date,
                jobs=plan_data["jobs"],
                selected_activities=plan_data["selected_activities"],
                overrides=plan_data.get("overrides")
            )
        
        # GlobalConfig ê°ì²´ë¡œ ë³€í™˜
        precedence_rules = []
        for rule in global_config.get("precedence", []):
            if isinstance(rule, (list, tuple)):
                precedence_rules.append(PrecedenceRule(
                    predecessor=rule[0],
                    successor=rule[1],
                    gap_min=rule[2] if len(rule) > 2 else 0,
                    is_adjacent=rule[3] if len(rule) > 3 else False
                ))
        
        # ìš´ì˜ì‹œê°„ ë³€í™˜
        op_hours = global_config.get("operating_hours", {})
        if isinstance(op_hours, dict) and "start" in op_hours:
            # ë‹¨ì¼ ì„¤ì •ì„ defaultë¡œ
            operating_hours = {
                "default": (
                    time.fromisoformat(op_hours["start"]),
                    time.fromisoformat(op_hours["end"])
                )
            }
        else:
            operating_hours = {"default": (time(9, 0), time(17, 30))}
        
        # batched ê·¸ë£¹ í¬ê¸° ë³€í™˜
        batched_sizes = {}
        for act, sizes in global_config.get("batched_group_sizes", {}).items():
            if isinstance(sizes, list) and len(sizes) >= 2:
                batched_sizes[act] = (sizes[0], sizes[1])
        
        global_config_obj = GlobalConfig(
            precedence_rules=precedence_rules,
            operating_hours=operating_hours,
            room_settings=rooms,  # ê·¸ëŒ€ë¡œ ì‚¬ìš©
            time_settings={act: data["duration_min"] for act, data in activities.items()},
            batched_group_sizes=batched_sizes,
            global_gap_min=global_config.get("global_gap_min", 5),
            max_stay_hours=global_config.get("max_stay_hours", 8)
        )
        
    except Exception as e:
        return {
            "status": "ERROR",
            "message": f"ì…ë ¥ ë°ì´í„° ë³€í™˜ ì˜¤ë¥˜: {str(e)}"
        }
    
    # 2. ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰
    scheduler = MultiDateScheduler(logger)
    
    # ê²€ì¦
    errors = scheduler.validate_config(date_plan_objects, global_config_obj)
    if errors:
        return {
            "status": "VALIDATION_ERROR",
            "errors": errors
        }
    
    # ìŠ¤ì¼€ì¤„ë§
    result = scheduler.schedule(
        date_plan_objects,
        global_config_obj,
        rooms,
        activities
    )
    
    # 3. ê²°ê³¼ ë°˜í™˜
    if result.status == "SUCCESS":
        # DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        df = result.to_dataframe()
        return {
            "status": "SUCCESS",
            "schedule": df,
            "summary": {
                "total_applicants": result.total_applicants,
                "scheduled_applicants": result.scheduled_applicants,
                "dates": len(result.results)
            }
        }
    else:
        # ì‹¤íŒ¨ ì •ë³´ ë°˜í™˜
        failed_info = {}
        for date, date_result in result.results.items():
            if date_result.status == "FAILED":
                failed_info[date.strftime("%Y-%m-%d")] = {
                    "error": date_result.error_message,
                    "logs": date_result.logs
                }
        
        return {
            "status": result.status,
            "scheduled_applicants": result.scheduled_applicants,
            "total_applicants": result.total_applicants,
            "failed_dates": [d.strftime("%Y-%m-%d") for d in result.failed_dates],
            "failed_info": failed_info,
            "partial_schedule": result.to_dataframe() if result.scheduled_applicants > 0 else None
        }


def convert_to_wide_format(schedule_df: pd.DataFrame) -> pd.DataFrame:
    """
    ìŠ¤ì¼€ì¤„ DataFrameì„ ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜ë˜ëŠ” wide í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        schedule_df: long í˜•ì‹ì˜ ìŠ¤ì¼€ì¤„ DataFrame
        
    Returns:
        wide í˜•ì‹ì˜ DataFrame (í™œë™ë³„ ì»¬ëŸ¼)
    """
    if schedule_df.empty:
        return pd.DataFrame()
    
    # í”¼ë²— í…Œì´ë¸” ìƒì„±
    wide_df = schedule_df.pivot_table(
        index=['id', 'interview_date'],
        columns='activity',
        values=['start_time', 'end_time', 'room'],
        aggfunc='first'
    ).reset_index()
    
    # ì»¬ëŸ¼ëª… í‰íƒ„í™”
    wide_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) and col[1] else col[0] 
                       for col in wide_df.columns]
    wide_df = wide_df.rename(columns={'id_': 'id', 'interview_date_': 'interview_date'})
    
    # ì‹œê°„ í˜•ì‹ ë³€í™˜
    time_cols = [col for col in wide_df.columns if col.startswith(('start_time_', 'end_time_'))]
    for col in time_cols:
        wide_df[col] = pd.to_datetime(wide_df[col]).dt.strftime('%H:%M')
    
    # ì»¬ëŸ¼ëª… ë³€ê²½ (ê¸°ì¡´ í˜•ì‹ì— ë§ê²Œ)
    rename_map = {}
    for col in wide_df.columns:
        if col.startswith('start_time_'):
            activity = col[len('start_time_'):]
            rename_map[col] = f'start_{activity}'
        elif col.startswith('end_time_'):
            activity = col[len('end_time_'):]
            rename_map[col] = f'end_{activity}'
        elif col.startswith('room_'):
            activity = col[len('room_'):]
            rename_map[col] = f'loc_{activity}'
    
    wide_df = wide_df.rename(columns=rename_map)
    
    # ì§ë¬´ ì½”ë“œ ì¶”ê°€
    wide_df['code'] = wide_df['id'].str.split('_').str[0]
    
    # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
    base_cols = ['id', 'interview_date', 'code']
    other_cols = sorted([col for col in wide_df.columns if col not in base_cols])
    
    return wide_df[base_cols + other_cols]


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_default_global_config() -> Dict:
    """ê¸°ë³¸ ì „ì—­ ì„¤ì • ìƒì„±"""
    return {
        "precedence": [],
        "operating_hours": {"start": "09:00", "end": "17:30"},
        "batched_group_sizes": {},
        "global_gap_min": 5,
        "max_stay_hours": 8
    }


def create_date_plan(
    date: str,
    jobs: Dict[str, int],
    activities: List[str],
    overrides: Optional[Dict] = None
) -> Dict:
    """ë‚ ì§œë³„ ê³„íš ìƒì„± í—¬í¼"""
    return {
        "date": date,
        "jobs": jobs,
        "selected_activities": activities,
        "overrides": overrides or {}
    }


def solve_for_days_v2(
    cfg_ui: dict, 
    params: dict = None, 
    debug: bool = False,
    progress_callback: Optional[ProgressCallback] = None
) -> Tuple[str, pd.DataFrame, str, int]:
    """
    ìƒˆë¡œìš´ ê³„ì¸µì  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ UI ìš”ì²­ ì²˜ë¦¬
    
    ê¸°ì¡´ solve_for_daysì™€ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤ ì œê³µ
    
    Args:
        cfg_ui: UI ì„¤ì • ë”•ì…”ë„ˆë¦¬
        params: ì¶”ê°€ íŒŒë¼ë¯¸í„°
        debug: ë””ë²„ê·¸ ëª¨ë“œ
        progress_callback: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
    
    Returns:
        (status, final_wide_df, logs, daily_limit)
    """
    try:
        # ë¡œê¹… ì„¤ì •
        log_level = logging.DEBUG if debug else logging.WARNING
        logging.basicConfig(level=log_level, format='%(levelname)s: %(message)s')
        logger = logging.getLogger(__name__)
        
        logs_buffer = []
        
        # ğŸš€ ìŠ¤ë§ˆíŠ¸ í†µí•© ë¡œì§ ì ìš©
        cfg_ui_optimized = _apply_smart_integration(cfg_ui, logs_buffer)
        
        # ìŠ¤ì¼€ì¤„ë§ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = SchedulingContext(
            progress_callback=progress_callback,
            debug=debug,
            time_limit_sec=params.get('time_limit_sec', 120.0)
        )
        
        # UI ë°ì´í„° ë³€í™˜
        date_plans, global_config, rooms, activities = _convert_ui_data(cfg_ui_optimized, logs_buffer)
        
        if not date_plans:
            return "FAILED", pd.DataFrame(), "ë‚ ì§œë³„ ê³„íšì´ ì—†ìŠµë‹ˆë‹¤.", 0
        
        # ë©€í‹° ë‚ ì§œ ìŠ¤ì¼€ì¤„ë§ ì‹¤í–‰
        scheduler = MultiDateScheduler()
        result = scheduler.schedule(
            date_plans=date_plans,
            global_config=global_config,
            rooms=rooms,
            activities=activities,
            context=context  # ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
        )
        
        # ê²°ê³¼ ë¶„ì„
        logs_buffer.append(f"=== ìŠ¤ì¼€ì¤„ë§ ê²°ê³¼ ===")
        logs_buffer.append(f"ì „ì²´ ìƒíƒœ: {result.status}")
        logs_buffer.append(f"ì´ ì§€ì›ì: {result.total_applicants}ëª…")
        logs_buffer.append(f"ìŠ¤ì¼€ì¤„ëœ ì§€ì›ì: {result.scheduled_applicants}ëª…")
        logs_buffer.append(f"ì„±ê³µë¥ : {result.scheduled_applicants/result.total_applicants*100:.1f}%")
        
        if result.status == "SUCCESS":
            # ì„±ê³µ - UI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            final_df = _convert_result_to_ui_format(result, logs_buffer)
            daily_limit = _calculate_daily_limit(result)
            
            return "SUCCESS", final_df, "\n".join(logs_buffer), daily_limit
            
        elif result.status == "PARTIAL":
            # ë¶€ë¶„ ì„±ê³µ
            logs_buffer.append(f"\nì‹¤íŒ¨í•œ ë‚ ì§œ: {len(result.failed_dates)}ê°œ")
            for failed_date in result.failed_dates:
                date_result = result.results[failed_date]
                logs_buffer.append(f"  - {failed_date.date()}: {date_result.error_message}")
            
            final_df = _convert_result_to_ui_format(result, logs_buffer)
            daily_limit = _calculate_daily_limit(result)
            
            return "PARTIAL", final_df, "\n".join(logs_buffer), daily_limit
            
        else:
            # ì™„ì „ ì‹¤íŒ¨
            logs_buffer.append("\nëª¨ë“  ë‚ ì§œ ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨")
            for failed_date in result.failed_dates:
                date_result = result.results[failed_date]
                logs_buffer.append(f"  - {failed_date.date()}: {date_result.error_message}")
            
            return "FAILED", pd.DataFrame(), "\n".join(logs_buffer), 0
    
    except Exception as e:
        logger.exception("ìŠ¤ì¼€ì¤„ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ")
        return "ERROR", pd.DataFrame(), f"ì˜ˆì™¸ ë°œìƒ: {str(e)}", 0


def _apply_smart_integration(cfg_ui: dict, logs_buffer: List[str]) -> dict:
    """
    ğŸš€ ìŠ¤ë§ˆíŠ¸ í†µí•© ë¡œì§: ì¸ì ‘ ì œì•½ì„ ìë™ ê°ì§€í•˜ì—¬ í™œë™ í†µí•©
    
    gap_min=0, adjacent=Trueì¸ ì„ í›„í–‰ ì œì•½ì„ ì°¾ì•„ì„œ
    í•´ë‹¹ í™œë™ë“¤ì„ ìë™ìœ¼ë¡œ í†µí•©í•˜ì—¬ ì—°ì†ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    """
    logs_buffer.append("=== ğŸš€ ìŠ¤ë§ˆíŠ¸ í†µí•© ë¡œì§ ì ìš© ===")
    
    # ì›ë³¸ ì„¤ì • ë³µì‚¬
    cfg_optimized = cfg_ui.copy()
    
    # ì„ í›„í–‰ ì œì•½ í™•ì¸
    precedence_df = cfg_ui.get("precedence", pd.DataFrame())
    if precedence_df.empty:
        logs_buffer.append("ì„ í›„í–‰ ì œì•½ ì—†ìŒ - í†µí•© ë¶ˆí•„ìš”")
        return cfg_optimized
    
    # ì¸ì ‘ ì œì•½ ì°¾ê¸° (gap_min=0, adjacent=True)
    adjacent_pairs = []
    for _, rule in precedence_df.iterrows():
        if rule.get("adjacent", False) and rule.get("gap_min", 0) == 0:
            pred = rule["predecessor"]
            succ = rule["successor"]
            adjacent_pairs.append((pred, succ))
            logs_buffer.append(f"ğŸ” ì¸ì ‘ ì œì•½ ë°œê²¬: {pred} â†’ {succ} (gap_min=0)")
    
    if not adjacent_pairs:
        logs_buffer.append("gap_min=0 ì¸ì ‘ ì œì•½ ì—†ìŒ - í†µí•© ë¶ˆí•„ìš”")
        return cfg_optimized
    
    # í™œë™ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    activities_df = cfg_ui.get("activities", pd.DataFrame()).copy()
    room_plan_df = cfg_ui.get("room_plan", pd.DataFrame()).copy()
    job_acts_map_df = cfg_ui.get("job_acts_map", pd.DataFrame()).copy()
    
    # ê° ì¸ì ‘ ìŒì— ëŒ€í•´ í†µí•© ì ìš©
    for pred, succ in adjacent_pairs:
        logs_buffer.append(f"ğŸ”§ {pred} + {succ} í†µí•© ì ìš© ì¤‘...")
        
        # í™œë™ ì •ë³´ ì°¾ê¸°
        pred_row = activities_df[activities_df["activity"] == pred]
        succ_row = activities_df[activities_df["activity"] == succ]
        
        if pred_row.empty or succ_row.empty:
            logs_buffer.append(f"âš ï¸ í™œë™ ì •ë³´ ì—†ìŒ: {pred} ë˜ëŠ” {succ}")
            continue
        
        pred_info = pred_row.iloc[0]
        succ_info = succ_row.iloc[0]
        
        # í†µí•© í™œë™ ìƒì„±
        integrated_name = f"{pred}+{succ}"
        integrated_duration = pred_info["duration_min"] + succ_info["duration_min"]
        integrated_room_type = succ_info["room_type"]  # í›„í–‰ í™œë™ì˜ ë°© ì‚¬ìš©
        integrated_mode = succ_info["mode"]  # í›„í–‰ í™œë™ì˜ ëª¨ë“œ ì‚¬ìš©
        integrated_capacity = succ_info["max_cap"]  # í›„í–‰ í™œë™ì˜ ìš©ëŸ‰ ì‚¬ìš©
        
        logs_buffer.append(f"  â†’ {integrated_name}({integrated_duration}ë¶„, {integrated_room_type}, {integrated_mode})")
        
        # ê¸°ì¡´ í™œë™ ì œê±°
        activities_df = activities_df[~activities_df["activity"].isin([pred, succ])]
        
        # í†µí•© í™œë™ ì¶”ê°€
        new_activity = {
            "use": True,
            "activity": integrated_name,
            "mode": integrated_mode,
            "duration_min": integrated_duration,
            "room_type": integrated_room_type,
            "min_cap": succ_info["min_cap"],
            "max_cap": integrated_capacity
        }
        activities_df = pd.concat([activities_df, pd.DataFrame([new_activity])], ignore_index=True)
        
        # ì„ í›„í–‰ ì œì•½ì—ì„œ í•´ë‹¹ ê·œì¹™ ì œê±°
        precedence_df = precedence_df[
            ~((precedence_df["predecessor"] == pred) & (precedence_df["successor"] == succ))
        ]
        
        # ì§€ì›ì í™œë™ ë§¤í•‘ ì—…ë°ì´íŠ¸
        if pred in job_acts_map_df.columns:
            job_acts_map_df = job_acts_map_df.drop(columns=[pred])
        if succ in job_acts_map_df.columns:
            job_acts_map_df = job_acts_map_df.drop(columns=[succ])
        
        # í†µí•© í™œë™ ì¶”ê°€
        job_acts_map_df[integrated_name] = True
        
        # ë°© ì„¤ì • ìµœì í™” (ì„ í–‰ í™œë™ ì „ìš© ë°© ì œê±°)
        pred_room_type = pred_info["room_type"]
        if pred_room_type != integrated_room_type:
            pred_count_col = f"{pred_room_type}_count"
            pred_cap_col = f"{pred_room_type}_cap"
            
            if pred_count_col in room_plan_df.columns:
                logs_buffer.append(f"  ë°© ì„¤ì • ìµœì í™”: {pred_room_type} ì œê±°")
                room_plan_df = room_plan_df.drop(columns=[pred_count_col], errors='ignore')
                room_plan_df = room_plan_df.drop(columns=[pred_cap_col], errors='ignore')
    
    # ìµœì í™”ëœ ì„¤ì • ë°˜í™˜
    cfg_optimized["activities"] = activities_df
    cfg_optimized["precedence"] = precedence_df
    cfg_optimized["job_acts_map"] = job_acts_map_df
    cfg_optimized["room_plan"] = room_plan_df
    
    logs_buffer.append(f"âœ… ìŠ¤ë§ˆíŠ¸ í†µí•© ì™„ë£Œ: {len(adjacent_pairs)}ê°œ í™œë™ ìŒ í†µí•©")
    
    return cfg_optimized


def _convert_ui_data(
    cfg_ui: dict, 
    logs_buffer: List[str]
) -> Tuple[Dict[datetime, DatePlan], GlobalConfig, Dict[str, dict], Dict[str, dict]]:
    """UI ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ ìŠ¤ì¼€ì¤„ëŸ¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    # 1. í™œë™ ì •ë³´ ì¶”ì¶œ
    activities_df = cfg_ui.get("activities", pd.DataFrame())
    if activities_df.empty:
        raise ValueError("í™œë™ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    activities = {}
    for _, row in activities_df.iterrows():
        if not row.get("use", False):
            continue
            
        activities[row["activity"]] = {
            "mode": row["mode"],
            "duration_min": int(row["duration_min"]),
            "room_type": row["room_type"],
            "min_capacity": int(row.get("min_cap", 1)),
            "max_capacity": int(row.get("max_cap", 1))
        }
    
    logs_buffer.append(f"í™œë™ {len(activities)}ê°œ ë¡œë“œ: {list(activities.keys())}")
    
    # 2. ë°© ì •ë³´ ì¶”ì¶œ  
    room_plan_df = cfg_ui.get("room_plan", pd.DataFrame())
    rooms = {}
    
    if not room_plan_df.empty:
        # room_planì—ì„œ ë°© íƒ€ì…ë³„ ê°œìˆ˜ì™€ ìš©ëŸ‰ ì¶”ì¶œ
        for room_type in activities_df["room_type"].dropna().unique():
            count_col = f"{room_type}_count"
            cap_col = f"{room_type}_cap"
            
            if count_col in room_plan_df.columns:
                count = int(room_plan_df[count_col].iloc[0])
                capacity = int(room_plan_df[cap_col].iloc[0]) if cap_col in room_plan_df.columns else 1
                
                rooms[room_type] = {
                    "count": count,
                    "capacity": capacity
                }
    
    logs_buffer.append(f"ë°© íƒ€ì… {len(rooms)}ê°œ: {rooms}")
    
    # 3. ì§ë¬´ë³„ í™œë™ ë§¤í•‘
    job_acts_df = cfg_ui.get("job_acts_map", pd.DataFrame())
    
    # 4. ë‚ ì§œë³„ ê³„íš ìƒì„±
    date_plans = {}
    
    # ìš´ì˜ ì‹œê°„ ì„¤ì •
    oper_window_df = cfg_ui.get("oper_window", pd.DataFrame())
    
    # ë©€í‹° ë‚ ì§œ ê³„íš ì²˜ë¦¬
    multidate_plans = cfg_ui.get("multidate_plans", {})
    
    if multidate_plans:
        # ìƒˆë¡œìš´ ë©€í‹° ë‚ ì§œ ê³„íš ë°©ì‹
        logs_buffer.append(f"ë©€í‹° ë‚ ì§œ ê³„íš {len(multidate_plans)}ê°œ ë°œê²¬")
        
        for date_key, plan in multidate_plans.items():
            if not plan.get("enabled", True):
                continue  # ë¹„í™œì„±í™”ëœ ë‚ ì§œëŠ” ê±´ë„ˆë›°ê¸°
            
            plan_date = plan["date"]
            if isinstance(plan_date, str):
                plan_date = datetime.strptime(plan_date, "%Y-%m-%d")
            elif hasattr(plan_date, 'date'):
                plan_date = datetime.combine(plan_date, datetime.min.time())
            else:
                plan_date = datetime.combine(plan_date, datetime.min.time())
            
            # í•´ë‹¹ ë‚ ì§œì˜ ì§ë¬´ë³„ ì¸ì›ìˆ˜
            jobs = {}
            for job in plan.get("jobs", []):
                jobs[job["code"]] = int(job["count"])
            
            # í•´ë‹¹ ì§ë¬´ê°€ ìˆ˜í–‰í•  í™œë™ë“¤ (ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  í™œë™)
            selected_activities = list(activities.keys())
            
            date_plans[plan_date] = DatePlan(
                date=plan_date,
                jobs=jobs,
                selected_activities=selected_activities
            )
            
            logs_buffer.append(f"ë‚ ì§œ ê³„íš ìƒì„±: {plan_date.date()}, ì§ë¬´: {jobs}, í™œë™: {selected_activities}")
            
    elif not job_acts_df.empty:
        # ê¸°ì¡´ ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
        logs_buffer.append("ê¸°ì¡´ job_acts_map ë°©ì‹ ì‚¬ìš©")
        
        # UIì—ì„œ ì„ íƒí•œ ë‚ ì§œë“¤ ê°€ì ¸ì˜¤ê¸°
        selected_dates = cfg_ui.get("interview_dates", [cfg_ui.get("interview_date")])
        if not selected_dates or selected_dates == [None]:
            # ê¸°ë³¸ê°’: ë‚´ì¼ ë‚ ì§œ
            selected_dates = [(datetime.now() + timedelta(days=1)).date()]
        
        # ê° ë‚ ì§œë³„ë¡œ DatePlan ìƒì„±
        for _, row in job_acts_df.iterrows():
            # ì§ë¬´ë³„ ì¸ì›ìˆ˜
            jobs = {row["code"]: int(row["count"])}
            
            # í•´ë‹¹ ì§ë¬´ê°€ ìˆ˜í–‰í•  í™œë™ë“¤
            selected_activities = []
            for activity_name in activities.keys():
                if row.get(activity_name, False):
                    selected_activities.append(activity_name)
            
            # ê° ì„ íƒëœ ë‚ ì§œë³„ë¡œ DatePlan ìƒì„±
            for selected_date in selected_dates:
                if isinstance(selected_date, str):
                    plan_date = datetime.strptime(selected_date, "%Y-%m-%d")
                elif hasattr(selected_date, 'date'):
                    plan_date = datetime.combine(selected_date, datetime.min.time())
                else:
                    plan_date = datetime.combine(selected_date, datetime.min.time())
                
                date_plans[plan_date] = DatePlan(
                    date=plan_date,
                    jobs=jobs,
                    selected_activities=selected_activities
                )
                
                logs_buffer.append(f"ë‚ ì§œ ê³„íš ìƒì„±: {plan_date.date()}, ì§ë¬´: {jobs}, í™œë™: {selected_activities}")
            break  # ì²« ë²ˆì§¸ ì§ë¬´ í–‰ë§Œ ì²˜ë¦¬ (ëª¨ë“  ë‚ ì§œì— ë™ì¼ ì ìš©)
    
    # 5. ì„ í›„í–‰ ì œì•½ ì¶”ì¶œ
    precedence_df = cfg_ui.get("precedence", pd.DataFrame())
    precedence_rules = []
    
    if not precedence_df.empty:
        for _, row in precedence_df.iterrows():
            if row["predecessor"] != "__START__" and row["successor"] != "__END__":
                precedence_rules.append(PrecedenceRule(
                    predecessor=row["predecessor"],
                    successor=row["successor"],
                    gap_min=int(row.get("gap_min", 5))
                ))
    
    # 6. ê¸€ë¡œë²Œ ì„¤ì •
    operating_hours = {"default": (time(9, 0), time(18, 0))}
    if not oper_window_df.empty:
        start_str = oper_window_df["start_time"].iloc[0]
        end_str = oper_window_df["end_time"].iloc[0]
        
        if isinstance(start_str, str):
            start_time = datetime.strptime(start_str, "%H:%M").time()
        else:
            start_time = start_str
            
        if isinstance(end_str, str):
            end_time = datetime.strptime(end_str, "%H:%M").time()
        else:
            end_time = end_str
            
        operating_hours["default"] = (start_time, end_time)
    
    # Batched ê·¸ë£¹ í¬ê¸° ì„¤ì •
    batched_group_sizes = {}
    for activity_name, activity_info in activities.items():
        if activity_info["mode"] == "batched":
            batched_group_sizes[activity_name] = (
                activity_info["min_capacity"],
                activity_info["max_capacity"]
            )
    
    global_config = GlobalConfig(
        operating_hours=operating_hours,
        precedence_rules=precedence_rules,
        batched_group_sizes=batched_group_sizes,
        room_settings={},
        time_settings={},
        global_gap_min=cfg_ui.get('global_gap_min', 5),
        max_stay_hours=cfg_ui.get('max_stay_hours', 5)  # ê¸°ë³¸ê°’ì„ 5ì‹œê°„ìœ¼ë¡œ ë³€ê²½
    )
    
    return date_plans, global_config, rooms, activities


def _convert_result_to_ui_format(result, logs_buffer: List[str]) -> pd.DataFrame:
    """ìŠ¤ì¼€ì¤„ë§ ê²°ê³¼ë¥¼ UI í‘œì‹œìš© DataFrameìœ¼ë¡œ ë³€í™˜"""
    
    schedule_data = []
    
    for date, date_result in result.results.items():
        if date_result.status != "SUCCESS":
            continue
            
        for item in date_result.schedule:
            # ë”ë¯¸ ì§€ì›ìëŠ” ì œì™¸
            if item.applicant_id.startswith("DUMMY_"):
                continue
                
            schedule_data.append({
                "interview_date": date,
                "applicant_id": item.applicant_id,
                "job_code": item.job_code,
                "activity_name": item.activity_name,
                "room_name": item.room_name,
                "start_time": item.start_time,
                "end_time": item.end_time,
                "duration_min": int((item.end_time - item.start_time).total_seconds() / 60)
            })
    
    if not schedule_data:
        return pd.DataFrame()
    
    df = pd.DataFrame(schedule_data)
    
    # ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
    df = df.sort_values(["interview_date", "start_time", "applicant_id"])
    
    logs_buffer.append(f"UI í˜•ì‹ ë³€í™˜ ì™„ë£Œ: {len(df)}ê°œ ìŠ¤ì¼€ì¤„ í•­ëª©")
    
    return df


def _calculate_daily_limit(result) -> int:
    """ì¼ì¼ ì²˜ë¦¬ ê°€ëŠ¥ ì¸ì› ê³„ì‚°"""
    max_daily = 0
    
    for date, date_result in result.results.items():
        if date_result.status == "SUCCESS":
            daily_count = len([
                item for item in date_result.schedule 
                if not item.applicant_id.startswith("DUMMY_")
            ])
            max_daily = max(max_daily, daily_count)
    
    return max_daily


def get_scheduler_comparison() -> Dict[str, Any]:
    """ë‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œìŠ¤í…œì˜ ë¹„êµ ì •ë³´ ì œê³µ"""
    return {
        "legacy": {
            "name": "OR-Tools ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬",
            "description": "Google OR-Tools CP-SAT ì œì•½ í•´ê²°ì‚¬ ì‚¬ìš©",
            "pros": ["ê°•ë ¥í•œ ì œì•½ í•´ê²°", "ìµœì í•´ ë³´ì¥"],
            "cons": ["ì„¤ì • ë³µì¡", "í° ë¬¸ì œì—ì„œ ëŠë¦¼"],
            "suitable_for": "ì†Œê·œëª¨, ë³µì¡í•œ ì œì•½"
        },
        "new": {
            "name": "ê³„ì¸µì  ìŠ¤ì¼€ì¤„ëŸ¬ v2",
            "description": "3ë‹¨ê³„ ê³„ì¸µì  ë¶„í•´ + íœ´ë¦¬ìŠ¤í‹± ë°©ì‹",
            "pros": ["ë¹ ë¥¸ ì²˜ë¦¬", "ëŒ€ê·œëª¨ ì²˜ë¦¬", "ì§ê´€ì  ì„¤ì •", "Batched í™œë™ ì§€ì›"],
            "cons": ["ìµœì í•´ ë¯¸ë³´ì¥"],
            "suitable_for": "ëŒ€ê·œëª¨, ì‹¤ì‹œê°„ ì²˜ë¦¬"
        }
    }


# ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜
def solve_for_days_hybrid(
    cfg_ui: dict, 
    params: dict = None, 
    debug: bool = False,
    use_new_scheduler: bool = True
) -> Tuple[str, pd.DataFrame, str, int]:
    """
    ë‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ ì„ íƒí•˜ì—¬ ì‹¤í–‰
    """
    if use_new_scheduler:
        return solve_for_days_v2(cfg_ui, params, debug)
    else:
        # ê¸°ì¡´ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
        from .solver import solve_for_days
        return solve_for_days(cfg_ui, params, debug)


def solve_for_days_optimized(
    cfg_ui: dict, 
    params: dict = None, 
    debug: bool = False,
    progress_callback: Optional[ProgressCallback] = None,
    optimization_config: Optional[Dict] = None
) -> Tuple[str, pd.DataFrame, str, int]:
    """
    ìµœì í™”ëœ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ UI ìš”ì²­ ì²˜ë¦¬
    ëŒ€ê·œëª¨ ì²˜ë¦¬ì— íŠ¹í™”ëœ ì„±ëŠ¥ ìµœì í™” ë²„ì „
    
    Args:
        cfg_ui: UI ì„¤ì • ë”•ì…”ë„ˆë¦¬
        params: ì¶”ê°€ íŒŒë¼ë¯¸í„°
        debug: ë””ë²„ê·¸ ëª¨ë“œ
        progress_callback: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
        optimization_config: ìµœì í™” ì„¤ì •
    
    Returns:
        (status, final_wide_df, logs, daily_limit)
    """
    try:
        # ë¡œê¹… ì„¤ì •
        log_level = logging.DEBUG if debug else logging.WARNING
        logging.basicConfig(level=log_level, format='%(levelname)s: %(message)s')
        logger = logging.getLogger(__name__)
        
        logs_buffer = []
        
        # ìµœì í™” ì„¤ì • ì²˜ë¦¬
        from .optimized_scheduler import OptimizationConfig
        
        if optimization_config:
            opt_config = OptimizationConfig(
                enable_parallel_processing=optimization_config.get("enable_parallel_processing", True),
                enable_memory_optimization=optimization_config.get("enable_memory_optimization", True),
                enable_caching=optimization_config.get("enable_caching", True),
                max_workers=optimization_config.get("max_workers"),
                chunk_size_threshold=optimization_config.get("chunk_size_threshold", 100),
                memory_cleanup_interval=optimization_config.get("memory_cleanup_interval", 50)
            )
        else:
            opt_config = OptimizationConfig()
        
        # ìŠ¤ì¼€ì¤„ë§ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = SchedulingContext(
            progress_callback=progress_callback,
            enable_detailed_logging=debug,
            real_time_updates=True
        )
        
        # UI ë°ì´í„° ë³€í™˜
        date_plans, global_config, rooms, activities = _convert_ui_data(cfg_ui, logs_buffer)
        
        if not date_plans:
            return "FAILED", pd.DataFrame(), "ë‚ ì§œë³„ ê³„íšì´ ì—†ìŠµë‹ˆë‹¤.", 0
        
        # ë©€í‹° ë‚ ì§œ ìŠ¤ì¼€ì¤„ë§ ì‹¤í–‰ (ìµœì í™”ëœ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©)
        from .optimized_multi_date_scheduler import OptimizedMultiDateScheduler
        
        scheduler = OptimizedMultiDateScheduler(optimization_config=opt_config)
        result = scheduler.schedule(
            date_plans=date_plans,
            global_config=global_config,
            rooms=rooms,
            activities=activities,
            context=context
        )
        
        # ê²°ê³¼ ë¶„ì„
        logs_buffer.append(f"=== ìµœì í™”ëœ ìŠ¤ì¼€ì¤„ë§ ê²°ê³¼ ===")
        logs_buffer.append(f"ì „ì²´ ìƒíƒœ: {result.status}")
        logs_buffer.append(f"ì´ ì§€ì›ì: {result.total_applicants}ëª…")
        logs_buffer.append(f"ìŠ¤ì¼€ì¤„ëœ ì§€ì›ì: {result.scheduled_applicants}ëª…")
        logs_buffer.append(f"ì„±ê³µë¥ : {result.scheduled_applicants/result.total_applicants*100:.1f}%")
        
        if hasattr(scheduler, 'stats'):
            stats = scheduler.get_overall_stats()
            logs_buffer.append(f"ìµœì í™” í†µê³„: ìºì‹œ ì ì¤‘ë¥  {stats.get('cache_hit_rate', 0):.1f}%, "
                             f"ë³‘ë ¬ ì‘ì—… {stats.get('parallel_tasks', 0)}ê°œ")
        
        if result.status == "SUCCESS":
            # ì„±ê³µ - UI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            final_df = _convert_result_to_ui_format(result, logs_buffer)
            daily_limit = _calculate_daily_limit(result)
            
            return "SUCCESS", final_df, "\n".join(logs_buffer), daily_limit
            
        elif result.status == "PARTIAL":
            # ë¶€ë¶„ ì„±ê³µ
            logs_buffer.append(f"\nì‹¤íŒ¨í•œ ë‚ ì§œ: {len(result.failed_dates)}ê°œ")
            for failed_date in result.failed_dates:
                date_result = result.results[failed_date]
                logs_buffer.append(f"  - {failed_date.date()}: {date_result.error_message}")
            
            final_df = _convert_result_to_ui_format(result, logs_buffer)
            daily_limit = _calculate_daily_limit(result)
            
            return "PARTIAL", final_df, "\n".join(logs_buffer), daily_limit
            
        else:
            # ì™„ì „ ì‹¤íŒ¨
            logs_buffer.append("\nëª¨ë“  ë‚ ì§œ ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨")
            for failed_date in result.failed_dates:
                date_result = result.results[failed_date]
                logs_buffer.append(f"  - {failed_date.date()}: {date_result.error_message}")
            
            return "FAILED", pd.DataFrame(), "\n".join(logs_buffer), 0
    
    except Exception as e:
        logger.exception("ìµœì í™”ëœ ìŠ¤ì¼€ì¤„ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ")
        return "ERROR", pd.DataFrame(), f"ì˜ˆì™¸ ë°œìƒ: {str(e)}", 0


def solve_for_days_two_phase(
    cfg_ui: dict, 
    params: dict = None, 
    debug: bool = False,
    progress_callback: Optional[ProgressCallback] = None,
    percentile: float = 90.0
) -> Tuple[str, pd.DataFrame, str, int, Dict[str, pd.DataFrame]]:
    """
    2ë‹¨ê³„ í•˜ë“œ ì œì•½ ìŠ¤ì¼€ì¤„ë§
    
    Args:
        cfg_ui: UI ì„¤ì • ë”•ì…”ë„ˆë¦¬
        params: ì¶”ê°€ íŒŒë¼ë¯¸í„°
        debug: ë””ë²„ê·¸ ëª¨ë“œ
        progress_callback: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
        percentile: í•˜ë“œ ì œì•½ ê³„ì‚°ìš© ë¶„ìœ„ìˆ˜ (ê¸°ë³¸ê°’: 90.0)
    
    Returns:
        (status, final_wide_df, logs, daily_limit, reports)
    """
    try:
        # ë¡œê¹… ì„¤ì •
        log_level = logging.DEBUG if debug else logging.WARNING
        logging.basicConfig(level=log_level, format='%(levelname)s: %(message)s')
        logger = logging.getLogger(__name__)
        
        logs_buffer = []
        
        # 2ë‹¨ê³„ í•˜ë“œ ì œì•½ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰
        from .hard_constraint_scheduler import HardConstraintScheduler
        
        scheduler = HardConstraintScheduler(percentile=percentile)
        result = scheduler.run_two_phase_scheduling(
            cfg_ui=cfg_ui,
            params=params,
            debug=debug,
            progress_callback=progress_callback
        )
        
        # ê²°ê³¼ ë¶„ì„
        logs_buffer.append(f"=== 2ë‹¨ê³„ í•˜ë“œ ì œì•½ ìŠ¤ì¼€ì¤„ë§ ê²°ê³¼ ===")
        logs_buffer.append(f"ì „ì²´ ìƒíƒœ: {result['status']}")
        
        if result['status'] == "SUCCESS":
            # ì„±ê³µ - UI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            final_df = result['phase2_result']
            
            # ì¼ì¼ ì²˜ë¦¬ ê°€ëŠ¥ ì¸ì› ê³„ì‚°
            if not final_df.empty:
                daily_limit = final_df.groupby('interview_date')['applicant_id'].nunique().max()
            else:
                daily_limit = 0
            
            # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
            reports = scheduler.generate_comprehensive_report(result)
            
            return "SUCCESS", final_df, "\n".join(logs_buffer), daily_limit, reports
            
        elif result['status'] == "PHASE1_FAILED":
            logs_buffer.append(f"1ë‹¨ê³„ ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨: {result['error']}")
            return "FAILED", pd.DataFrame(), "\n".join(logs_buffer), 0, {}
            
        elif result['status'] == "ANALYSIS_FAILED":
            logs_buffer.append(f"ì²´ë¥˜ì‹œê°„ ë¶„ì„ ì‹¤íŒ¨: {result['error']}")
            return "FAILED", pd.DataFrame(), "\n".join(logs_buffer), 0, {}
            
        elif result['status'] == "PHASE2_FAILED":
            logs_buffer.append(f"2ë‹¨ê³„ ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨: {result['error']}")
            # 1ë‹¨ê³„ ê²°ê³¼ë¼ë„ ë°˜í™˜
            final_df = result['phase1_result']
            if not final_df.empty:
                daily_limit = final_df.groupby('interview_date')['applicant_id'].nunique().max()
            else:
                daily_limit = 0
            
            reports = {}
            if result.get('constraint_analysis'):
                reports['constraint_analysis'] = result['constraint_analysis']
            
            return "PARTIAL", final_df, "\n".join(logs_buffer), daily_limit, reports
            
        else:
            logs_buffer.append(f"ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ: {result['status']}")
            return "FAILED", pd.DataFrame(), "\n".join(logs_buffer), 0, {}
    
    except Exception as e:
        logger.exception("2ë‹¨ê³„ í•˜ë“œ ì œì•½ ìŠ¤ì¼€ì¤„ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ")
        return "ERROR", pd.DataFrame(), f"ì˜ˆì™¸ ë°œìƒ: {str(e)}", 0, {} 